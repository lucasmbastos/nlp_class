{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import gensim\n",
    "import tqdm\n",
    "import nltk\n",
    "\n",
    "# Torch Stuff\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset Utility Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset, word_vector_path, vector_dimension):\n",
    "        super(torch.utils.data.Dataset, self).__init__()\n",
    "\n",
    "        self.__vector_dimension = vector_dimension\n",
    "\n",
    "        self.__dataset = self.__prepare_dataset(dataset)\n",
    "        self.__word_vector = self.__read_word_vector(word_vector_path)\n",
    "\n",
    "        self.__length_of_instances = len(self.__dataset['REVIEW'].iloc[0])\n",
    "        \n",
    "\n",
    "    def __prepare_dataset(self, dataset):\n",
    "        dataset_copy = dataset.copy(deep=True)\n",
    "        return dataset_copy\n",
    "    \n",
    "\n",
    "    def __read_word_vector(self, path):\n",
    "        word2vec_model = word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(path)\n",
    "\n",
    "        word2vec_model.wv[\"<pad>\"] = np.array(\n",
    "            [0.01]*self.__vector_dimension, dtype=np.float)\n",
    "        word2vec_model.wv[\"<end>\"] = np.array(\n",
    "            [0.02]*self.__vector_dimension, dtype=np.float)\n",
    "\n",
    "        return word2vec_model.wv\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__dataset)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.__dataset.iloc[idx]\n",
    "\n",
    "        review = instance[0]\n",
    "        score = instance[1]\n",
    "\n",
    "        embedded_review = self.__embed_review(review)\n",
    "\n",
    "        return (embedded_review, score)\n",
    "\n",
    "    def __embed_review(self, review):\n",
    "        return np.array(list(map(self.__get_embedding, review)))\n",
    "    \n",
    "\n",
    "    def __get_embedding(self, word):\n",
    "        try:\n",
    "            return self.__word_vector[word]\n",
    "        except KeyError:\n",
    "            return np.array([0]*self.__vector_dimension, dtype=np.float)\n",
    "        \n",
    "\n",
    "    def get_length_of_reviews(self):\n",
    "        return self.__length_of_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilitary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def pre_processing(review):\n",
    "    review = review.lower()\n",
    "    review = cleanhtml(review)\n",
    "    return tokenizer.tokenize(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading from Processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_dataset = pd.read_csv(\"../data/processed/acllib_data.csv\")\n",
    "usable_dataset = usable_dataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50000.000000\n",
       "mean       234.139260\n",
       "std        173.495615\n",
       "min          6.000000\n",
       "25%        128.000000\n",
       "50%        176.000000\n",
       "75%        284.000000\n",
       "max       2487.000000\n",
       "Name: REVIEW, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(\"[a-z]+\")\n",
    "\n",
    "usable_dataset[\"REVIEW\"] = usable_dataset[\"REVIEW\"].map(pre_processing)\n",
    "usable_dataset[\"SCORE\"] = usable_dataset[\"SCORE\"].map(lambda x: 0 if x <5 else 1)\n",
    "usable_dataset.head()\n",
    "\n",
    "### Fill sentences\n",
    "\n",
    "usable_dataset[\"REVIEW\"].map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = usable_dataset[\"REVIEW\"].map(len).describe()['75%'] + 1\n",
    "max_sentence_length\n",
    "\n",
    "def fill_sentence(sentence):\n",
    "    tokens_to_fill = int(max_sentence_length - len(sentence))\n",
    "    \n",
    "    sentence.append('<END>')\n",
    "    sentence.extend(['<PAD>']*tokens_to_fill)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_dataset[\"REVIEW\"] = usable_dataset[\"REVIEW\"].map(fill_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe, test_dataframe = sklearn.model_selection.train_test_split(usable_dataset,\n",
    "                                             test_size=0.33,\n",
    "                                             shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Dataset Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miranda/Documents/academico/mestrado/monitoria/workshop/venv/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/miranda/Documents/academico/mestrado/monitoria/workshop/venv/lib/python3.6/site-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/home/miranda/Documents/academico/mestrado/monitoria/workshop/venv/lib/python3.6/site-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "train_data = IMDBDataset(train_dataframe, \"../data/processed/glove.6b.300d.txt\", 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DArchitecture(nn.Module):\n",
    "    def __init__(self, number_of_tokens, embedding_size):\n",
    "        super(Conv1DArchitecture, self).__init__()\n",
    "        \n",
    "        self.number_of_tokens = number_of_tokens\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        number_of_kernels = 100\n",
    "        \n",
    "\n",
    "        self.convolution_window_2 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=number_of_kernels,\n",
    "            kernel_size=(2, embedding_size),\n",
    "        )\n",
    "        \n",
    "        self.convolution_window_3 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=number_of_kernels,\n",
    "            kernel_size=(3, embedding_size),\n",
    "        )\n",
    "        \n",
    "        self.convolution_window_4 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=number_of_kernels,\n",
    "            kernel_size=(4, embedding_size),\n",
    "        )\n",
    "        \n",
    "        self.max_pooling_window_2 = torch.nn.MaxPool1d(kernel_size=number_of_tokens-1)\n",
    "        self.max_pooling_window_3 = torch.nn.MaxPool1d(kernel_size=number_of_tokens-2)\n",
    "        self.max_pooling_window_4 = torch.nn.MaxPool1d(kernel_size=number_of_tokens-3)\n",
    "        \n",
    "        self.dense_1 = torch.nn.Linear(300, 150)\n",
    "        self.dense_2 = torch.nn.Linear(150, 75)\n",
    "        self.dense_3 = torch.nn.Linear(75, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(-1,1,x.shape[1], self.embedding_size)\n",
    "\n",
    "        x_window2 = F.relu(self.convolution_window_2(x).squeeze(-1))\n",
    "        x_window3 = F.relu(self.convolution_window_3(x).squeeze(-1))\n",
    "        x_window4 = F.relu(self.convolution_window_4(x).squeeze(-1))\n",
    "        \n",
    "        \n",
    "        x_window2 = self.max_pooling_window_2(x_window2).squeeze(-1)\n",
    "        x_window3 = self.max_pooling_window_3(x_window3).squeeze(-1)\n",
    "        x_window4 = self.max_pooling_window_4(x_window4).squeeze(-1)\n",
    "        \n",
    "        y = torch.cat((x_window2, x_window3, x_window4), 1)\n",
    "        \n",
    "        y = F.dropout(F.relu(self.dense_1(y)),0.5)\n",
    "        y = F.dropout(F.relu(self.dense_2(y)),0.5)\n",
    "        y = F.relu(self.dense_3(y))\n",
    "\n",
    "        return y.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciating Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Conv1DArchitecture(\n",
      "  (convolution_window_2): Conv2d(1, 100, kernel_size=(2, 300), stride=(1, 1))\n",
      "  (convolution_window_3): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
      "  (convolution_window_4): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
      "  (max_pooling_window_2): MaxPool1d(kernel_size=284.0, stride=284.0, padding=0, dilation=1, ceil_mode=False)\n",
      "  (max_pooling_window_3): MaxPool1d(kernel_size=283.0, stride=283.0, padding=0, dilation=1, ceil_mode=False)\n",
      "  (max_pooling_window_4): MaxPool1d(kernel_size=282.0, stride=282.0, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dense_1): Linear(in_features=300, out_features=150, bias=True)\n",
      "  (dense_2): Linear(in_features=150, out_features=75, bias=True)\n",
      "  (dense_3): Linear(in_features=75, out_features=1, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = Conv1DArchitecture(max_sentence_length, embedding_size=300)\n",
    "net = net.to(device)\n",
    "print(net.parameters)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/miranda/Documents/academico/mestrado/monitoria/workshop/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/miranda/Documents/academico/mestrado/monitoria/workshop/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 187, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/miranda/Documents/academico/mestrado/monitoria/workshop/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 187, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/miranda/Documents/academico/mestrado/monitoria/workshop/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 173, in default_collate\n    return torch.stack([torch.from_numpy(b) for b in batch], 0)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 286 and 359 in dimension 1 at /pytorch/aten/src/TH/generic/THTensorMath.cpp:3616\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-17ed20221c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0macc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/academico/mestrado/monitoria/workshop/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/academico/mestrado/monitoria/workshop/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"/home/miranda/Documents/academico/mestrado/monitoria/workshop/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/miranda/Documents/academico/mestrado/monitoria/workshop/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 187, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/miranda/Documents/academico/mestrado/monitoria/workshop/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 187, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/home/miranda/Documents/academico/mestrado/monitoria/workshop/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 173, in default_collate\n    return torch.stack([torch.from_numpy(b) for b in batch], 0)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 286 and 359 in dimension 1 at /pytorch/aten/src/TH/generic/THTensorMath.cpp:3616\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    acc_loss = []\n",
    "    for entry, labels in torch.utils.data.DataLoader(train_data, num_workers=4, batch_size=32):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        entry = entry.type(torch.FloatTensor)\n",
    "        labels = labels.type(torch.FloatTensor)\n",
    "        entry = entry.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = net(entry)\n",
    "\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc_loss.append(np.float(loss))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    print(\"Error on test: %.4f\" % np.mean(acc_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data\n",
    "test_data = IMDBDataset(test_dataframe, \"../data/processed/glove.6b.300d.txt\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t_outputs = torch.tensor([], dtype=torch.float32)\n",
    "    t_labels = torch.tensor([], dtype=torch.float32)\n",
    "\n",
    "    for entry_test, labels_test in torch.utils.data.DataLoader(test_data, num_workers=4, batch_size=100000):\n",
    "\n",
    "        entry_test = entry_test.float()\n",
    "        labels_test = labels_test.float()\n",
    "        entry_test = entry_test.to(device)\n",
    "        labels_test = labels_test.to(device)\n",
    "        \n",
    "        t_labels = torch.cat((t_labels, labels_test),0)\n",
    "        t_outputs = torch.cat((t_outputs, net(entry_test)),0)\n",
    "\n",
    "    validation_error = F.mse_loss(t_outputs, t_labels)\n",
    "    print(\"Error on test: %.4f\" % validation_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
