{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_in_list(operation, target_list):\n",
    "    return list(map(operation,target_list))\n",
    "\n",
    "def load_file(path):\n",
    "    with open(path, 'rt') as file_pointer:\n",
    "        lines = file_pointer.readlines()\n",
    "        lines = apply_in_list(lambda x: x.strip(), lines)\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Theme in Yellow by Carl Sandburg',\n",
       " 'I spot the hills',\n",
       " 'With yellow balls in autumn.',\n",
       " 'I light the prairie cornfields',\n",
       " 'Orange and tawny gold clusters',\n",
       " 'And I am called pumpkins.',\n",
       " 'On the last of October',\n",
       " 'When dusk is fallen',\n",
       " 'Children join hands',\n",
       " 'And circle round me',\n",
       " 'Singing ghost songs',\n",
       " 'And love to the harvest moon;',\n",
       " 'I am...',\n",
       " '',\n",
       " 'October by May Swenson',\n",
       " 'A smudge for the horizon',\n",
       " 'that, on a clear day, shows',\n",
       " 'the hard edge of hills and',\n",
       " 'buildings on the other coast.',\n",
       " 'Anchored boats all head one way:',\n",
       " 'north, where the wind comes from.',\n",
       " 'You can see the storm inflating',\n",
       " 'out of the west. A dark hole',\n",
       " 'in...']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems_lines = load_file(\"../data/raw/poems\")\n",
    "poems_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['h',\n",
       "  'e',\n",
       "  'm',\n",
       "  'e',\n",
       "  'i',\n",
       "  'n',\n",
       "  'e',\n",
       "  'l',\n",
       "  'l',\n",
       "  'o',\n",
       "  'w',\n",
       "  'b',\n",
       "  'y',\n",
       "  'a',\n",
       "  'r',\n",
       "  'l',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd',\n",
       "  'b',\n",
       "  'u',\n",
       "  'r',\n",
       "  'g'],\n",
       " ['s', 'p', 'o', 't', 't', 'h', 'e', 'h', 'i', 'l', 'l', 's'],\n",
       " ['i',\n",
       "  't',\n",
       "  'h',\n",
       "  'y',\n",
       "  'e',\n",
       "  'l',\n",
       "  'l',\n",
       "  'o',\n",
       "  'w',\n",
       "  'b',\n",
       "  'a',\n",
       "  'l',\n",
       "  'l',\n",
       "  's',\n",
       "  'i',\n",
       "  'n',\n",
       "  'a',\n",
       "  'u',\n",
       "  't',\n",
       "  'u',\n",
       "  'm',\n",
       "  'n'],\n",
       " ['l',\n",
       "  'i',\n",
       "  'g',\n",
       "  'h',\n",
       "  't',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'p',\n",
       "  'r',\n",
       "  'a',\n",
       "  'i',\n",
       "  'r',\n",
       "  'i',\n",
       "  'e',\n",
       "  'c',\n",
       "  'o',\n",
       "  'r',\n",
       "  'n',\n",
       "  'f',\n",
       "  'i',\n",
       "  'e',\n",
       "  'l',\n",
       "  'd',\n",
       "  's'],\n",
       " ['r',\n",
       "  'a',\n",
       "  'n',\n",
       "  'g',\n",
       "  'e',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd',\n",
       "  't',\n",
       "  'a',\n",
       "  'w',\n",
       "  'n',\n",
       "  'y',\n",
       "  'g',\n",
       "  'o',\n",
       "  'l',\n",
       "  'd',\n",
       "  'c',\n",
       "  'l',\n",
       "  'u',\n",
       "  's',\n",
       "  't',\n",
       "  'e',\n",
       "  'r',\n",
       "  's'],\n",
       " ['n',\n",
       "  'd',\n",
       "  'a',\n",
       "  'm',\n",
       "  'c',\n",
       "  'a',\n",
       "  'l',\n",
       "  'l',\n",
       "  'e',\n",
       "  'd',\n",
       "  'p',\n",
       "  'u',\n",
       "  'm',\n",
       "  'p',\n",
       "  'k',\n",
       "  'i',\n",
       "  'n',\n",
       "  's'],\n",
       " ['n',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'l',\n",
       "  'a',\n",
       "  's',\n",
       "  't',\n",
       "  'o',\n",
       "  'f',\n",
       "  'c',\n",
       "  't',\n",
       "  'o',\n",
       "  'b',\n",
       "  'e',\n",
       "  'r'],\n",
       " ['h', 'e', 'n', 'd', 'u', 's', 'k', 'i', 's', 'f', 'a', 'l', 'l', 'e', 'n'],\n",
       " ['h',\n",
       "  'i',\n",
       "  'l',\n",
       "  'd',\n",
       "  'r',\n",
       "  'e',\n",
       "  'n',\n",
       "  'j',\n",
       "  'o',\n",
       "  'i',\n",
       "  'n',\n",
       "  'h',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd',\n",
       "  's'],\n",
       " ['n', 'd', 'c', 'i', 'r', 'c', 'l', 'e', 'r', 'o', 'u', 'n', 'd', 'm', 'e'],\n",
       " ['i',\n",
       "  'n',\n",
       "  'g',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  'g',\n",
       "  'h',\n",
       "  'o',\n",
       "  's',\n",
       "  't',\n",
       "  's',\n",
       "  'o',\n",
       "  'n',\n",
       "  'g',\n",
       "  's'],\n",
       " ['n',\n",
       "  'd',\n",
       "  'l',\n",
       "  'o',\n",
       "  'v',\n",
       "  'e',\n",
       "  't',\n",
       "  'o',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'h',\n",
       "  'a',\n",
       "  'r',\n",
       "  'v',\n",
       "  'e',\n",
       "  's',\n",
       "  't',\n",
       "  'm',\n",
       "  'o',\n",
       "  'o',\n",
       "  'n'],\n",
       " ['a', 'm'],\n",
       " [],\n",
       " ['c',\n",
       "  't',\n",
       "  'o',\n",
       "  'b',\n",
       "  'e',\n",
       "  'r',\n",
       "  'b',\n",
       "  'y',\n",
       "  'a',\n",
       "  'y',\n",
       "  'w',\n",
       "  'e',\n",
       "  'n',\n",
       "  's',\n",
       "  'o',\n",
       "  'n'],\n",
       " ['s',\n",
       "  'm',\n",
       "  'u',\n",
       "  'd',\n",
       "  'g',\n",
       "  'e',\n",
       "  'f',\n",
       "  'o',\n",
       "  'r',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'h',\n",
       "  'o',\n",
       "  'r',\n",
       "  'i',\n",
       "  'z',\n",
       "  'o',\n",
       "  'n'],\n",
       " ['t',\n",
       "  'h',\n",
       "  'a',\n",
       "  't',\n",
       "  'o',\n",
       "  'n',\n",
       "  'a',\n",
       "  'c',\n",
       "  'l',\n",
       "  'e',\n",
       "  'a',\n",
       "  'r',\n",
       "  'd',\n",
       "  'a',\n",
       "  'y',\n",
       "  's',\n",
       "  'h',\n",
       "  'o',\n",
       "  'w',\n",
       "  's'],\n",
       " ['t',\n",
       "  'h',\n",
       "  'e',\n",
       "  'h',\n",
       "  'a',\n",
       "  'r',\n",
       "  'd',\n",
       "  'e',\n",
       "  'd',\n",
       "  'g',\n",
       "  'e',\n",
       "  'o',\n",
       "  'f',\n",
       "  'h',\n",
       "  'i',\n",
       "  'l',\n",
       "  'l',\n",
       "  's',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd'],\n",
       " ['b',\n",
       "  'u',\n",
       "  'i',\n",
       "  'l',\n",
       "  'd',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  's',\n",
       "  'o',\n",
       "  'n',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'o',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  'c',\n",
       "  'o',\n",
       "  'a',\n",
       "  's',\n",
       "  't'],\n",
       " ['n',\n",
       "  'c',\n",
       "  'h',\n",
       "  'o',\n",
       "  'r',\n",
       "  'e',\n",
       "  'd',\n",
       "  'b',\n",
       "  'o',\n",
       "  'a',\n",
       "  't',\n",
       "  's',\n",
       "  'a',\n",
       "  'l',\n",
       "  'l',\n",
       "  'h',\n",
       "  'e',\n",
       "  'a',\n",
       "  'd',\n",
       "  'o',\n",
       "  'n',\n",
       "  'e',\n",
       "  'w',\n",
       "  'a',\n",
       "  'y'],\n",
       " ['n',\n",
       "  'o',\n",
       "  'r',\n",
       "  't',\n",
       "  'h',\n",
       "  'w',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  'e',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'w',\n",
       "  'i',\n",
       "  'n',\n",
       "  'd',\n",
       "  'c',\n",
       "  'o',\n",
       "  'm',\n",
       "  'e',\n",
       "  's',\n",
       "  'f',\n",
       "  'r',\n",
       "  'o',\n",
       "  'm'],\n",
       " ['o',\n",
       "  'u',\n",
       "  'c',\n",
       "  'a',\n",
       "  'n',\n",
       "  's',\n",
       "  'e',\n",
       "  'e',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  's',\n",
       "  't',\n",
       "  'o',\n",
       "  'r',\n",
       "  'm',\n",
       "  'i',\n",
       "  'n',\n",
       "  'f',\n",
       "  'l',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g'],\n",
       " ['o',\n",
       "  'u',\n",
       "  't',\n",
       "  'o',\n",
       "  'f',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'w',\n",
       "  'e',\n",
       "  's',\n",
       "  't',\n",
       "  'd',\n",
       "  'a',\n",
       "  'r',\n",
       "  'k',\n",
       "  'h',\n",
       "  'o',\n",
       "  'l',\n",
       "  'e'],\n",
       " ['i', 'n']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(\"[a-z]\")\n",
    "\n",
    "tokenized_lines_1 = apply_in_list(lambda x: tokenizer.tokenize(x), poems_lines)\n",
    "tokenized_lines_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Theme', 'in', 'Yellow', 'by', 'Carl', 'Sandburg'],\n",
       " ['I', 'spot', 'the', 'hills'],\n",
       " ['With', 'yellow', 'balls', 'in', 'autumn'],\n",
       " ['I', 'light', 'the', 'prairie', 'cornfields'],\n",
       " ['Orange', 'and', 'tawny', 'gold', 'clusters'],\n",
       " ['And', 'I', 'am', 'called', 'pumpkins'],\n",
       " ['On', 'the', 'last', 'of', 'October'],\n",
       " ['When', 'dusk', 'is', 'fallen'],\n",
       " ['Children', 'join', 'hands'],\n",
       " ['And', 'circle', 'round', 'me'],\n",
       " ['Singing', 'ghost', 'songs'],\n",
       " ['And', 'love', 'to', 'the', 'harvest', 'moon'],\n",
       " ['I', 'am'],\n",
       " [],\n",
       " ['October', 'by', 'May', 'Swenson'],\n",
       " ['A', 'smudge', 'for', 'the', 'horizon'],\n",
       " ['that', 'on', 'a', 'clear', 'day', 'shows'],\n",
       " ['the', 'hard', 'edge', 'of', 'hills', 'and'],\n",
       " ['buildings', 'on', 'the', 'other', 'coast'],\n",
       " ['Anchored', 'boats', 'all', 'head', 'one', 'way'],\n",
       " ['north', 'where', 'the', 'wind', 'comes', 'from'],\n",
       " ['You', 'can', 'see', 'the', 'storm', 'inflating'],\n",
       " ['out', 'of', 'the', 'west', 'A', 'dark', 'hole'],\n",
       " ['in']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(\"[a-zA-Z0-9]+\")\n",
    "\n",
    "tokenized_lines_2 = apply_in_list(lambda x: tokenizer.tokenize(x), poems_lines)\n",
    "tokenized_lines_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['theme', 'in', 'yellow', 'by', 'carl', 'sandburg'],\n",
       " ['i', 'spot', 'the', 'hills'],\n",
       " ['with', 'yellow', 'balls', 'in', 'autumn'],\n",
       " ['i', 'light', 'the', 'prairie', 'cornfields'],\n",
       " ['orange', 'and', 'tawny', 'gold', 'clusters'],\n",
       " ['and', 'i', 'am', 'called', 'pumpkins'],\n",
       " ['on', 'the', 'last', 'of', 'october'],\n",
       " ['when', 'dusk', 'is', 'fallen'],\n",
       " ['children', 'join', 'hands'],\n",
       " ['and', 'circle', 'round', 'me'],\n",
       " ['singing', 'ghost', 'songs'],\n",
       " ['and', 'love', 'to', 'the', 'harvest', 'moon'],\n",
       " ['i', 'am'],\n",
       " [],\n",
       " ['october', 'by', 'may', 'swenson'],\n",
       " ['a', 'smudge', 'for', 'the', 'horizon'],\n",
       " ['that', 'on', 'a', 'clear', 'day', 'shows'],\n",
       " ['the', 'hard', 'edge', 'of', 'hills', 'and'],\n",
       " ['buildings', 'on', 'the', 'other', 'coast'],\n",
       " ['anchored', 'boats', 'all', 'head', 'one', 'way'],\n",
       " ['north', 'where', 'the', 'wind', 'comes', 'from'],\n",
       " ['you', 'can', 'see', 'the', 'storm', 'inflating'],\n",
       " ['out', 'of', 'the', 'west', 'a', 'dark', 'hole'],\n",
       " ['in']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems_lines = apply_in_list(lambda x: x.lower(), poems_lines)\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(\"[a-zA-Z0-9]+\")\n",
    "\n",
    "tokenized_lines = apply_in_list(lambda x: tokenizer.tokenize(x), poems_lines)\n",
    "tokenized_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ha'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"has\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['theme', 'in', 'yellow', 'by', 'carl', 'sandburg']\n",
      "['i', 'spot', 'the', 'hill']\n",
      "['with', 'yellow', 'ball', 'in', 'autumn']\n",
      "['i', 'light', 'the', 'prairi', 'cornfield']\n",
      "['orang', 'and', 'tawni', 'gold', 'cluster']\n",
      "['and', 'i', 'am', 'call', 'pumpkin']\n",
      "['on', 'the', 'last', 'of', 'octob']\n",
      "['when', 'dusk', 'is', 'fallen']\n",
      "['children', 'join', 'hand']\n",
      "['and', 'circl', 'round', 'me']\n",
      "['sing', 'ghost', 'song']\n",
      "['and', 'love', 'to', 'the', 'harvest', 'moon']\n",
      "['i', 'am']\n",
      "[]\n",
      "['octob', 'by', 'may', 'swenson']\n",
      "['a', 'smudg', 'for', 'the', 'horizon']\n",
      "['that', 'on', 'a', 'clear', 'day', 'show']\n",
      "['the', 'hard', 'edg', 'of', 'hill', 'and']\n",
      "['build', 'on', 'the', 'other', 'coast']\n",
      "['anchor', 'boat', 'all', 'head', 'one', 'way']\n",
      "['north', 'where', 'the', 'wind', 'come', 'from']\n",
      "['you', 'can', 'see', 'the', 'storm', 'inflat']\n",
      "['out', 'of', 'the', 'west', 'a', 'dark', 'hole']\n",
      "['in']\n"
     ]
    }
   ],
   "source": [
    "for tokenized_line in tokenized_lines:\n",
    "    print(apply_in_list(stemmer.stem, tokenized_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowball Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/home/miranda/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/miranda/Documents/mestrado/monitoria/python_class/demonstrations/venv/nltk_data'\n    - '/home/miranda/Documents/mestrado/monitoria/python_class/demonstrations/venv/share/nltk_data'\n    - '/home/miranda/Documents/mestrado/monitoria/python_class/demonstrations/venv/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/mestrado/monitoria/python_class/demonstrations/venv/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/mestrado/monitoria/python_class/demonstrations/venv/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/home/miranda/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/miranda/Documents/mestrado/monitoria/python_class/demonstrations/venv/nltk_data'\n    - '/home/miranda/Documents/mestrado/monitoria/python_class/demonstrations/venv/share/nltk_data'\n    - '/home/miranda/Documents/mestrado/monitoria/python_class/demonstrations/venv/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7c3248a945d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/mestrado/monitoria/python_class/demonstrations/venv/lib/python3.6/site-packages/nltk/stem/snowball.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, ignore_stopwords)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The language '{0}' is not supported.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mstemmerclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Stemmer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmerclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/mestrado/monitoria/python_class/demonstrations/venv/lib/python3.6/site-packages/nltk/stem/snowball.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ignore_stopwords)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_stopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/mestrado/monitoria/python_class/demonstrations/venv/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/mestrado/monitoria/python_class/demonstrations/venv/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/mestrado/monitoria/python_class/demonstrations/venv/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/mestrado/monitoria/python_class/demonstrations/venv/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/home/miranda/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/miranda/Documents/mestrado/monitoria/python_class/demonstrations/venv/nltk_data'\n    - '/home/miranda/Documents/mestrado/monitoria/python_class/demonstrations/venv/share/nltk_data'\n    - '/home/miranda/Documents/mestrado/monitoria/python_class/demonstrations/venv/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer('english', ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokenized_line in tokenized_lines:\n",
    "    print(apply_in_list(stemmer.stem, tokenized_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'city',\n",
       "  'of',\n",
       "  'new',\n",
       "  'york',\n",
       "  'often',\n",
       "  'called',\n",
       "  'new',\n",
       "  'york',\n",
       "  'city',\n",
       "  'nyc',\n",
       "  'or',\n",
       "  'simply',\n",
       "  'new',\n",
       "  'york',\n",
       "  'ny',\n",
       "  'is',\n",
       "  'the',\n",
       "  'most',\n",
       "  'populous',\n",
       "  'city',\n",
       "  'in',\n",
       "  'the',\n",
       "  'united',\n",
       "  'states',\n",
       "  'with',\n",
       "  'an',\n",
       "  'estimated',\n",
       "  '2017',\n",
       "  'population',\n",
       "  'of',\n",
       "  '8',\n",
       "  '622',\n",
       "  '698',\n",
       "  'distributed',\n",
       "  'over',\n",
       "  'a',\n",
       "  'land',\n",
       "  'area',\n",
       "  'of',\n",
       "  'about',\n",
       "  '302',\n",
       "  '6',\n",
       "  'square',\n",
       "  'miles',\n",
       "  '784',\n",
       "  'km2',\n",
       "  'new',\n",
       "  'york',\n",
       "  'city',\n",
       "  'is',\n",
       "  'also',\n",
       "  'the',\n",
       "  'most',\n",
       "  'densely',\n",
       "  'populated',\n",
       "  'major',\n",
       "  'city',\n",
       "  'in',\n",
       "  'the',\n",
       "  'united',\n",
       "  'states',\n",
       "  'located',\n",
       "  'at',\n",
       "  'the',\n",
       "  'southern',\n",
       "  'tip',\n",
       "  'of',\n",
       "  'the',\n",
       "  'state',\n",
       "  'of',\n",
       "  'new',\n",
       "  'york',\n",
       "  'the',\n",
       "  'city',\n",
       "  'is',\n",
       "  'the',\n",
       "  'center',\n",
       "  'of',\n",
       "  'the',\n",
       "  'new',\n",
       "  'york',\n",
       "  'metropolitan',\n",
       "  'area',\n",
       "  'the',\n",
       "  'largest',\n",
       "  'metropolitan',\n",
       "  'area',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'by',\n",
       "  'urban',\n",
       "  'landmass',\n",
       "  'and',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  's',\n",
       "  'most',\n",
       "  'populous',\n",
       "  'megacities',\n",
       "  'with',\n",
       "  'an',\n",
       "  'estimated',\n",
       "  '20',\n",
       "  '320',\n",
       "  '876',\n",
       "  'people',\n",
       "  'in',\n",
       "  'its',\n",
       "  '2017',\n",
       "  'metropolitan',\n",
       "  'statistical',\n",
       "  'area',\n",
       "  'and',\n",
       "  '23',\n",
       "  '876',\n",
       "  '155',\n",
       "  'residents',\n",
       "  'in',\n",
       "  'its',\n",
       "  'combined',\n",
       "  'statistical',\n",
       "  'area',\n",
       "  'a',\n",
       "  'global',\n",
       "  'power',\n",
       "  'city',\n",
       "  'new',\n",
       "  'york',\n",
       "  'city',\n",
       "  'has',\n",
       "  'been',\n",
       "  'described',\n",
       "  'uniquely',\n",
       "  'as',\n",
       "  'the',\n",
       "  'cultural',\n",
       "  'financial',\n",
       "  'and',\n",
       "  'media',\n",
       "  'capital',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  'and',\n",
       "  'exerts',\n",
       "  'a',\n",
       "  'significant',\n",
       "  'impact',\n",
       "  'upon',\n",
       "  'commerce',\n",
       "  'entertainment',\n",
       "  'research',\n",
       "  'technology',\n",
       "  'education',\n",
       "  'politics',\n",
       "  'tourism',\n",
       "  'art',\n",
       "  'fashion',\n",
       "  'and',\n",
       "  'sports',\n",
       "  'the',\n",
       "  'city',\n",
       "  's',\n",
       "  'fast',\n",
       "  'pace',\n",
       "  'has',\n",
       "  'inspired',\n",
       "  'the',\n",
       "  'term',\n",
       "  'new',\n",
       "  'york',\n",
       "  'minute',\n",
       "  'home',\n",
       "  'to',\n",
       "  'the',\n",
       "  'headquarters',\n",
       "  'of',\n",
       "  'the',\n",
       "  'united',\n",
       "  'nations',\n",
       "  'new',\n",
       "  'york',\n",
       "  'is',\n",
       "  'an',\n",
       "  'important',\n",
       "  'center',\n",
       "  'for',\n",
       "  'international',\n",
       "  'diplomacy'],\n",
       " [],\n",
       " ['situated',\n",
       "  'on',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  's',\n",
       "  'largest',\n",
       "  'natural',\n",
       "  'harbors',\n",
       "  'new',\n",
       "  'york',\n",
       "  'city',\n",
       "  'consists',\n",
       "  'of',\n",
       "  'five',\n",
       "  'boroughs',\n",
       "  'each',\n",
       "  'of',\n",
       "  'which',\n",
       "  'is',\n",
       "  'a',\n",
       "  'separate',\n",
       "  'county',\n",
       "  'of',\n",
       "  'the',\n",
       "  'state',\n",
       "  'of',\n",
       "  'new',\n",
       "  'york',\n",
       "  'the',\n",
       "  'five',\n",
       "  'boroughs',\n",
       "  'brooklyn',\n",
       "  'queens',\n",
       "  'manhattan',\n",
       "  'the',\n",
       "  'bronx',\n",
       "  'and',\n",
       "  'staten',\n",
       "  'island',\n",
       "  'were',\n",
       "  'consolidated',\n",
       "  'into',\n",
       "  'a',\n",
       "  'single',\n",
       "  'city',\n",
       "  'in',\n",
       "  '1898',\n",
       "  'the',\n",
       "  'city',\n",
       "  'and',\n",
       "  'its',\n",
       "  'metropolitan',\n",
       "  'area',\n",
       "  'constitute',\n",
       "  'the',\n",
       "  'premier',\n",
       "  'gateway',\n",
       "  'for',\n",
       "  'legal',\n",
       "  'immigration',\n",
       "  'to',\n",
       "  'the',\n",
       "  'united',\n",
       "  'states',\n",
       "  'as',\n",
       "  'many',\n",
       "  'as',\n",
       "  '800',\n",
       "  'languages',\n",
       "  'are',\n",
       "  'spoken',\n",
       "  'in',\n",
       "  'new',\n",
       "  'york',\n",
       "  'making',\n",
       "  'it',\n",
       "  'the',\n",
       "  'most',\n",
       "  'linguistically',\n",
       "  'diverse',\n",
       "  'city',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'new',\n",
       "  'york',\n",
       "  'city',\n",
       "  'is',\n",
       "  'home',\n",
       "  'to',\n",
       "  'more',\n",
       "  'than',\n",
       "  '3',\n",
       "  '2',\n",
       "  'million',\n",
       "  'residents',\n",
       "  'born',\n",
       "  'outside',\n",
       "  'the',\n",
       "  'united',\n",
       "  'states',\n",
       "  'the',\n",
       "  'largest',\n",
       "  'foreign',\n",
       "  'born',\n",
       "  'population',\n",
       "  'of',\n",
       "  'any',\n",
       "  'city',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'in',\n",
       "  '2017',\n",
       "  'the',\n",
       "  'new',\n",
       "  'york',\n",
       "  'metropolitan',\n",
       "  'area',\n",
       "  'produced',\n",
       "  'a',\n",
       "  'gross',\n",
       "  'metropolitan',\n",
       "  'product',\n",
       "  'gmp',\n",
       "  'of',\n",
       "  'us',\n",
       "  '1',\n",
       "  '73',\n",
       "  'trillion',\n",
       "  'if',\n",
       "  'greater',\n",
       "  'new',\n",
       "  'york',\n",
       "  'city',\n",
       "  'were',\n",
       "  'a',\n",
       "  'sovereign',\n",
       "  'state',\n",
       "  'it',\n",
       "  'would',\n",
       "  'have',\n",
       "  'the',\n",
       "  '12th',\n",
       "  'highest',\n",
       "  'gdp',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world'],\n",
       " [],\n",
       " ['new',\n",
       "  'york',\n",
       "  'city',\n",
       "  'traces',\n",
       "  'its',\n",
       "  'origins',\n",
       "  'to',\n",
       "  'a',\n",
       "  'trading',\n",
       "  'post',\n",
       "  'founded',\n",
       "  'by',\n",
       "  'colonists',\n",
       "  'from',\n",
       "  'the',\n",
       "  'dutch',\n",
       "  'republic',\n",
       "  'in',\n",
       "  '1624',\n",
       "  'on',\n",
       "  'lower',\n",
       "  'manhattan',\n",
       "  'the',\n",
       "  'post',\n",
       "  'was',\n",
       "  'named',\n",
       "  'new',\n",
       "  'amsterdam',\n",
       "  'in',\n",
       "  '1626',\n",
       "  'the',\n",
       "  'city',\n",
       "  'and',\n",
       "  'its',\n",
       "  'surroundings',\n",
       "  'came',\n",
       "  'under',\n",
       "  'english',\n",
       "  'control',\n",
       "  'in',\n",
       "  '1664',\n",
       "  'and',\n",
       "  'were',\n",
       "  'renamed',\n",
       "  'new',\n",
       "  'york',\n",
       "  'after',\n",
       "  'king',\n",
       "  'charles',\n",
       "  'ii',\n",
       "  'of',\n",
       "  'england',\n",
       "  'granted',\n",
       "  'the',\n",
       "  'lands',\n",
       "  'to',\n",
       "  'his',\n",
       "  'brother',\n",
       "  'the',\n",
       "  'duke',\n",
       "  'of',\n",
       "  'york',\n",
       "  'new',\n",
       "  'york',\n",
       "  'served',\n",
       "  'as',\n",
       "  'the',\n",
       "  'capital',\n",
       "  'of',\n",
       "  'the',\n",
       "  'united',\n",
       "  'states',\n",
       "  'from',\n",
       "  '1785',\n",
       "  'until',\n",
       "  '1790',\n",
       "  'it',\n",
       "  'has',\n",
       "  'been',\n",
       "  'the',\n",
       "  'country',\n",
       "  's',\n",
       "  'largest',\n",
       "  'city',\n",
       "  'since',\n",
       "  '1790',\n",
       "  'the',\n",
       "  'statue',\n",
       "  'of',\n",
       "  'liberty',\n",
       "  'greeted',\n",
       "  'millions',\n",
       "  'of',\n",
       "  'immigrants',\n",
       "  'as',\n",
       "  'they',\n",
       "  'came',\n",
       "  'to',\n",
       "  'the',\n",
       "  'americas',\n",
       "  'by',\n",
       "  'ship',\n",
       "  'in',\n",
       "  'the',\n",
       "  'late',\n",
       "  '19th',\n",
       "  'and',\n",
       "  'early',\n",
       "  '20th',\n",
       "  'centuries',\n",
       "  'and',\n",
       "  'is',\n",
       "  'a',\n",
       "  'world',\n",
       "  'symbol',\n",
       "  'of',\n",
       "  'the',\n",
       "  'united',\n",
       "  'states',\n",
       "  'and',\n",
       "  'its',\n",
       "  'ideals',\n",
       "  'of',\n",
       "  'liberty',\n",
       "  'and',\n",
       "  'peace',\n",
       "  'in',\n",
       "  'the',\n",
       "  '21st',\n",
       "  'century',\n",
       "  'new',\n",
       "  'york',\n",
       "  'has',\n",
       "  'emerged',\n",
       "  'as',\n",
       "  'a',\n",
       "  'global',\n",
       "  'node',\n",
       "  'of',\n",
       "  'creativity',\n",
       "  'and',\n",
       "  'entrepreneurship',\n",
       "  'social',\n",
       "  'tolerance',\n",
       "  'and',\n",
       "  'environmental',\n",
       "  'sustainability',\n",
       "  'and',\n",
       "  'as',\n",
       "  'a',\n",
       "  'symbol',\n",
       "  'of',\n",
       "  'freedom',\n",
       "  'and',\n",
       "  'cultural',\n",
       "  'diversity'],\n",
       " [],\n",
       " ['many',\n",
       "  'districts',\n",
       "  'and',\n",
       "  'landmarks',\n",
       "  'in',\n",
       "  'new',\n",
       "  'york',\n",
       "  'city',\n",
       "  'are',\n",
       "  'well',\n",
       "  'known',\n",
       "  'with',\n",
       "  'the',\n",
       "  'city',\n",
       "  'having',\n",
       "  'three',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  's',\n",
       "  'ten',\n",
       "  'most',\n",
       "  'visited',\n",
       "  'tourist',\n",
       "  'attractions',\n",
       "  'in',\n",
       "  '2013',\n",
       "  'and',\n",
       "  'receiving',\n",
       "  'a',\n",
       "  'record',\n",
       "  '62',\n",
       "  '8',\n",
       "  'million',\n",
       "  'tourists',\n",
       "  'in',\n",
       "  '2017',\n",
       "  'several',\n",
       "  'sources',\n",
       "  'have',\n",
       "  'ranked',\n",
       "  'new',\n",
       "  'york',\n",
       "  'the',\n",
       "  'most',\n",
       "  'photographed',\n",
       "  'city',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'times',\n",
       "  'square',\n",
       "  'iconic',\n",
       "  'as',\n",
       "  'the',\n",
       "  'world',\n",
       "  's',\n",
       "  'heart',\n",
       "  'and',\n",
       "  'its',\n",
       "  'crossroads',\n",
       "  'is',\n",
       "  'the',\n",
       "  'brightly',\n",
       "  'illuminated',\n",
       "  'hub',\n",
       "  'of',\n",
       "  'the',\n",
       "  'broadway',\n",
       "  'theater',\n",
       "  'district',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  's',\n",
       "  'busiest',\n",
       "  'pedestrian',\n",
       "  'intersections',\n",
       "  'and',\n",
       "  'a',\n",
       "  'major',\n",
       "  'center',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  's',\n",
       "  'entertainment',\n",
       "  'industry',\n",
       "  'the',\n",
       "  'names',\n",
       "  'of',\n",
       "  'many',\n",
       "  'of',\n",
       "  'the',\n",
       "  'city',\n",
       "  's',\n",
       "  'landmarks',\n",
       "  'skyscrapers',\n",
       "  'and',\n",
       "  'parks',\n",
       "  'are',\n",
       "  'known',\n",
       "  'around',\n",
       "  'the',\n",
       "  'world',\n",
       "  'manhattan',\n",
       "  's',\n",
       "  'real',\n",
       "  'estate',\n",
       "  'market',\n",
       "  'is',\n",
       "  'among',\n",
       "  'the',\n",
       "  'most',\n",
       "  'expensive',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'new',\n",
       "  'york',\n",
       "  'is',\n",
       "  'home',\n",
       "  'to',\n",
       "  'the',\n",
       "  'largest',\n",
       "  'ethnic',\n",
       "  'chinese',\n",
       "  'population',\n",
       "  'outside',\n",
       "  'of',\n",
       "  'asia',\n",
       "  'with',\n",
       "  'multiple',\n",
       "  'signature',\n",
       "  'chinatowns',\n",
       "  'developing',\n",
       "  'across',\n",
       "  'the',\n",
       "  'city',\n",
       "  'providing',\n",
       "  'continuous',\n",
       "  '24',\n",
       "  '7',\n",
       "  'service',\n",
       "  'the',\n",
       "  'new',\n",
       "  'york',\n",
       "  'city',\n",
       "  'subway',\n",
       "  'is',\n",
       "  'the',\n",
       "  'largest',\n",
       "  'single',\n",
       "  'operator',\n",
       "  'rapid',\n",
       "  'transit',\n",
       "  'system',\n",
       "  'worldwide',\n",
       "  'with',\n",
       "  '472',\n",
       "  'rail',\n",
       "  'stations',\n",
       "  'over',\n",
       "  '120',\n",
       "  'colleges',\n",
       "  'and',\n",
       "  'universities',\n",
       "  'are',\n",
       "  'located',\n",
       "  'in',\n",
       "  'new',\n",
       "  'york',\n",
       "  'city',\n",
       "  'including',\n",
       "  'columbia',\n",
       "  'university',\n",
       "  'new',\n",
       "  'york',\n",
       "  'university',\n",
       "  'and',\n",
       "  'rockefeller',\n",
       "  'university',\n",
       "  'which',\n",
       "  'have',\n",
       "  'been',\n",
       "  'ranked',\n",
       "  'among',\n",
       "  'the',\n",
       "  'top',\n",
       "  'universities',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'anchored',\n",
       "  'by',\n",
       "  'wall',\n",
       "  'street',\n",
       "  'in',\n",
       "  'the',\n",
       "  'financial',\n",
       "  'district',\n",
       "  'of',\n",
       "  'lower',\n",
       "  'manhattan',\n",
       "  'it',\n",
       "  'has',\n",
       "  'been',\n",
       "  'called',\n",
       "  'both',\n",
       "  'the',\n",
       "  'most',\n",
       "  'economically',\n",
       "  'powerful',\n",
       "  'city',\n",
       "  'and',\n",
       "  'the',\n",
       "  'leading',\n",
       "  'financial',\n",
       "  'center',\n",
       "  'of',\n",
       "  'the',\n",
       "  'world',\n",
       "  'and',\n",
       "  'the',\n",
       "  'city',\n",
       "  'is',\n",
       "  'home',\n",
       "  'to',\n",
       "  'the',\n",
       "  'world',\n",
       "  's',\n",
       "  'two',\n",
       "  'largest',\n",
       "  'stock',\n",
       "  'exchanges',\n",
       "  'by',\n",
       "  'total',\n",
       "  'market',\n",
       "  'capitalization',\n",
       "  'the',\n",
       "  'new',\n",
       "  'york',\n",
       "  'stock',\n",
       "  'exchange',\n",
       "  'and',\n",
       "  'nasdaq']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_york_corpus = load_file(\"../data/raw/new_york\")\n",
    "new_york_corpus = apply_in_list(lambda x: tokenizer.tokenize(x.lower()), new_york_corpus)\n",
    "new_york_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases and Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.phrases.Phrases at 0x7f101a599358>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_new_york_phrases = gensim.models.phrases.Phrases(new_york_corpus)\n",
    "bigram_new_york_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'city', 'of', 'new_york']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_new_york_phrases[['the', 'city', 'of', 'new', 'york']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x7f101a5adb00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_new_york_phrases[new_york_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_new_york_corpus = bigram_new_york_phrases[new_york_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_york_word_2_vec = gensim.models.word2vec.Word2Vec(bigram_new_york_corpus, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.8539616465568542),\n",
       " ('in', 0.7426537275314331),\n",
       " ('city', 0.7309365272521973),\n",
       " ('world', 0.7116335034370422),\n",
       " ('and', 0.6912716627120972),\n",
       " ('of', 0.6751328110694885),\n",
       " ('a', 0.6435642242431641),\n",
       " ('area', 0.6405653357505798),\n",
       " ('metropolitan', 0.6210491061210632),\n",
       " ('s', 0.6185368299484253)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_york_word_2_vec.wv.similar_by_vector(\"new_york\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00072319,  0.00551608, -0.0093502 ,  0.00344164, -0.0035011 ,\n",
       "       -0.00381537,  0.00027762,  0.0038802 ,  0.00937217,  0.00313875,\n",
       "        0.00657089,  0.0069883 , -0.00163839,  0.01002916,  0.0048383 ,\n",
       "        0.00850541, -0.00472094,  0.00202631,  0.00888059, -0.00606367,\n",
       "        0.00592949,  0.00377793,  0.0090571 ,  0.00723883, -0.00571491,\n",
       "        0.01253568,  0.0057293 , -0.00930411, -0.00547404, -0.00225993,\n",
       "        0.00558491,  0.00621172, -0.00781286, -0.00046917, -0.00426969,\n",
       "        0.00259661,  0.00475539, -0.00086822,  0.00091551, -0.00057263,\n",
       "        0.00797326,  0.00142464, -0.00390889,  0.00677817, -0.00465158,\n",
       "        0.00238811,  0.00083496, -0.00154922, -0.00367749, -0.0071643 ,\n",
       "       -0.01009379, -0.00652849,  0.00706138,  0.00173057,  0.00513783,\n",
       "       -0.00229316, -0.0013963 , -0.00347413, -0.00186482, -0.00654995,\n",
       "        0.00410566,  0.01207789, -0.00821588,  0.00100448,  0.00273469,\n",
       "        0.00863728, -0.00143037,  0.00056936, -0.00221569, -0.00763784,\n",
       "       -0.0065392 ,  0.00249379, -0.00779844,  0.00325695,  0.00777586,\n",
       "       -0.00384915, -0.00218148, -0.00440093,  0.0058834 , -0.00287944,\n",
       "        0.01043159, -0.00676335, -0.00270327,  0.00296265,  0.00449654,\n",
       "       -0.00365302,  0.00069626,  0.00309623,  0.00056969, -0.0024921 ,\n",
       "        0.00812238, -0.00714698, -0.01107091,  0.00406759,  0.00578762,\n",
       "        0.00486355,  0.00202677,  0.00296904, -0.00196703, -0.00644868],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_york_word_2_vec.wv['new_york']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
